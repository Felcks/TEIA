{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Entropia \n",
    "\n",
    "produzido por : Leandro G. M. Alvim, DCC/UFRRJ \n",
    "\n",
    "$H(X) = -\\sum_i^{n}{ P(x_i)log_2(P(x_i))} $\n",
    "\n",
    "Entropia é um conceito muito estudado e antigo que vem da termodinâmica introduzido por Rudolf Clausius. Em 1948, foi abordado no campo da teoria da informação por Claude Shannon no trabalho intitulado $\\textit{A Mathematical Theory of Communication}$. Veremos aqui o conceito de entropia de Shanon.\n",
    "\n",
    "Podemos entender a entropia de duas formas: como medida de incerteza ou como medida de impureza. Ambas estas medidas estão associadas à previsão.\n",
    "\n",
    "Vejamos um exemplo que associa estes conceitos:\n",
    "\n",
    "Exemplo 1)\n",
    "\n",
    "Imagine que você tenha uma bacia com duas substâncias que não se misturam, água e óleo, e você queira tirar uma amostra (num copo) desta bacia, com a condição de que uma amostra só é válida quando só existe uma substância. Desta forma, as possibilidades são duas: o copo ter somente óleo ou o copo ter somente água. Imagine agora que você queira desenvolver um sistema que faça uma previsão de qual substância existe na sua amostra: água ou óleo.\n",
    "\n",
    "Bom, podemos notar que nosso sistema é limitado às condições de mistura de óleo e água da bacia. Por exemplo, se a bacia apresenta 50% de óleo e água (impureza máxima), é impossível prever com qualidade a substância. Desta forma, a entropia  é alta (incerteza é alta). \n",
    "\n",
    "Agora imagine que nesta bacia temos apenas água. Com apenas aǵua (impureza baixa), prever a substância da amostra fica fácil. Ou seja, a incerteza é reduzida a zero.\n",
    "\n",
    "Exemplo 2)\n",
    "\n",
    "Caso 1) Imagine que você deseja saber aonde um professor está localizado na instituição e existe uma probabilidade igual dele estar em quaisquer uma das salas do instituto. Difícil de encontrá-lo, não? Entropia alta (incerteza alta). \n",
    "\n",
    "Caso 2) Agora imagine que lhe dou uma informação acerca deste professor. Digo que ele está localizado no bloco de informática do instituto (existem 4 blocos). Sabemos agora que a incerteza (entropia) de encontrar este professor foi reduzida.\n",
    "\n",
    "Como poderíamos quantificar esta incerteza?\n",
    "\n",
    "Assumindo que existem oito salas por bloco, tal que são quatro blocos tal que só existe um andar. Resslatando, novamente, que os eventos são equiprováveis. Para o caso 1, em que ele pode estar em qualquer bloco ou sala, mediríamos a entropia como $log_2(N)$, tal que N é o número total de salas (estados). Desta forma, a incerteza corresponde a $log_2(32) = 5$. Já para o caso 2, onde sabemos que o professor se encontra no bloco de informática (8 salas possíveis), a entropia é $log_2(8) = 3$. Veja que interessante, A entropia foi reduzida quando lhe dei uma informação para o problema e o número de possibilidades reduziu a incerteza!\n",
    "\n",
    "Bom, ok, mas e a fórmula com um somatório?\n",
    "\n",
    "A fórmula abaixo, com um somatório, foi desenvolvida para trabalhar com eventos tanto equiprováveis quanto não-equiprováveis.\n",
    "\n",
    "Vejamos como a fórmula se encaixa para esta situação de eventos equiprováveis:\n",
    "\n",
    "Seja N o número de estados (salas), e sabendo que o professor se encontra no bloco de informática, tal que existe probabilidades iguais do professor em uma das oitos salas.\n",
    "\n",
    "$H(X) = -\\sum_i^{n}{ P(x_i)log_2(P(x_i))} = [-(1/8)*log_2(1/8) - (1/8)*log_2(1/8)... ]$\n",
    "\n",
    "$= 8*[-(1/8)*log_2(1/8)] = -log_2(1/8) = -[log_2(1) -log_2(8)] = -[0 - 3] = 3$\n",
    "\n",
    "Veja, chegamos ao mesmo resultado!\n",
    "\n",
    "Agora, imagine que exista uma probabilidade maior deste mesmo professor estar em uma duas salas. 26%, 26% e 8%  nas demais salas.\n",
    "\n",
    "$H(X) = -\\sum_i^{n}{ P(x_i)log_2(P(x_i))} = [-2*(.26)*log_2(.26) - 6*(.08)*log_2(.08)] = 2.75$\n",
    "\n",
    "Veja que interessante, com a informação de que duas salas são mais prováveis do que as demais, a incerteza (entropia) reduziu de 3 (caso equiprovável anterior) para 2.75!\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def H(X):\n",
    "    \n",
    "    entropy = 0\n",
    "    for px in X:\n",
    "        entropy -= px*np.log2(px)\n",
    "    return entropy    \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exemplo da moeda não-viciada:  \n",
    "Incerteza máxima. Impossível prever! Totalmente aleatório"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "H([.5,.5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exemplo da moeda extremamente viciada \n",
    "Incerteza baixa. Fácil prever!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.080793135895911181"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "H([.01,.99])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exemplo de Shannon: Canal Binário com Símbolos Equiprováveis\n",
    "Imagine uma mensagem num canal binário, com possíveis símbolos $A$, $B$, $C$ e $D$ tal estes símbolos são equiprováveis $P(A)=P(B)=P(C)=P(D)$. Qual será a entropia? Impossível prever um símbolo! incerteza Máxima! curiosamente, o resultado é número médio de bits esperado se estas letras fossem transmitidas num canal. Para cada símbolo, a representação seria: $A=00$,$B=01$,$C=10$, $D=11$. Média $= 0.25*2 + 0.25*2 + 0.25*2 + 0.25*2$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.0"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "H([1/4,1/4,1/4,1/4])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exemplo de Shannon: Canal Binário com Símbolos Não Equiprováveis\n",
    "imagine agora uma mensagem num canal binário, com possíveis símbolos $A$, $B$, $C$ e $D$ \n",
    "tal estes símbolos não são mais equiprováveis $P(A)=1/2$, $P(B)=1/4$, $P(C)=1/8$, $P(D)=1/8$. Qual será a entropia? \n",
    "Mais fácil de prever que o exemplo anterior, não?\n",
    "Lembre-se de huffman. Símbolos mais frequentes possuem codificação menor que símbolos menos frequentes para minimizar o tamanho médio da informação. \n",
    "Como algumas são mais frequentes, consigo compactá-las para a seguinte forma: $A=0$, $B=10$, $C=110$, $D=111$. \n",
    "Curiosamente, o resultado é número médio de bits esperado se estas letras fossem transmitidas num canal.\n",
    "A média = $.50*1+.25*2+0.125*3+0.125*3$ \n",
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.75"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "H([1/2,1/4,1/8,1/8])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Voltando a fórmula \n",
    "Sobre a interpretação de Shannon\n",
    "\n",
    "$H(X) = -\\sum_{i=1}^{n}{ P(x_i)log(P(x_i))} $ que significa a esperança da informação $H(X) = E(I(X)) = E[-log(P(X))]$\n",
    "Assim, entendemos que $-log(P(X))$ mede a quantidade de informação e $P(X)$ informa com que frequência aquela informação ocorre. O somatório com as probabilidades acaba sendo uma média "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Entropia Condicional\n",
    "$H(Y|X) = \\sum_{i=1}^{n}{ P(x_i)H(Y|x_i)} $ \n",
    "\n",
    "Traduzindo: queremos quantificar a entropia de Y, dado que X foi selecionado.\n",
    "\n",
    "Nada mais é do que a esperaça (média) das entropias de $Y$ dado que $x_i$ ocorreu.\n",
    "Vejamos um exemplo para os seguintes dados:\n",
    "\n",
    "$\n",
    "\\begin{array}{rrrr} \\hline\n",
    "\\textbf{Umidade} & \\textbf{Tempo} & \\textbf{Evento} \\\\ \\hline\n",
    "Alta & Sol & Não Choveu  \\\\ \\hline\n",
    "Alta & Sol &     Choveu  \\\\ \\hline\n",
    "Alta & Nublado & Choveu  \\\\ \\hline\n",
    "\\end{array}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Revisando Probabilidades $P(X)$\n",
    "Sobre as probabilidades da umidade:\n",
    "\n",
    "A probabilidade de umidade alta $P(Umidade=Alta)= 1$.\n",
    "\n",
    "\n",
    "Sobre as probabilidades do tempo:\n",
    "\n",
    "A probabilidade de estar sol $P(Tempo=Sol)= 1/3$ e de estar nublado $P(Tempo=Nublado)=2/3$.\n",
    "\n",
    "Sobre as probabilidades do evento:\n",
    "\n",
    "A probabilidade de chover $P(Evento=Chover)= 2/3$ e de não chover $P(Evento=NãoChover)=1/3$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Revisando Probabilidades Condicionais $P(Y|X)$\n",
    "Sobre as probabilidades do evento dado o tempo:\n",
    "\n",
    "Para $Tempo=Sol$ temos:\n",
    "\n",
    "A probabilidade de chover dado que está sol é $P(Evento=Chover|Tempo=Sol)= 0.5$.\n",
    "\n",
    "A probabilidade de não chover dado que está sol é $P(Evento=NãoChover|Tempo=Sol)= 0.5$.\n",
    "\n",
    "Para $Tempo=Nublado$ temos:\n",
    "\n",
    "A probabilidade de chover dado que está nublado é $P(Evento=Chover|Tempo=Nublado)= 1$.\n",
    "\n",
    "A probabilidade de não chover dado que está nublado é $P(Evento=NãoChover|Nublado)= 0$.\n",
    "\n",
    "Para $Umidade=Alta$ temos:\n",
    "\n",
    "A probabilidade de chover dado que está úmido é $P(Evento=Chover|Umidade=Alta)= 1$.\n",
    "\n",
    "A probabilidade de não chover dado que está nublado é $P(Evento=NãoChover|Umidade=Alta)= 0$.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Qual o valor de atributo mais informativo?\n",
    "O atributo Umidade não agrega nenhuma informação quanto a previsão de chuva!\n",
    "Já o atributo tempo agrega alguma. Por exempo, quando está nublado, temos certeza (para esta amostra) de que vai chover!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Para tomar esta decisão precisamos medir o quanto cada atributo é informativo\n",
    "\n",
    "Como cada atributo possui valores que podem ser distintos, vamos tirar a média da entropia para cada conjunto de valores.\n",
    "\n",
    "Vejamos um exemplo para calcular $H(Evento|Tempo)$.\n",
    "\n",
    "Temos que $H(Evento|Tempo) = P(Tempo=Sol)*H(Evento|Tempo=Sol) + P(Tempo=Nublado)*H(Evento|Tempo=Nublado) $\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Calculando cada termo condicional separado\n",
    "\n",
    "Calculamos $P(Tempo=Sol)*H(Evento|Tempo=Sol)$ como \n",
    "\n",
    "$= P(Tempo=Sol)[ -P(Evento=Chover|Tempo=Sol)*log(P(Evento=Chover|Tempo=Sol) - P(Evento=NãoChover|Tempo=Sol)*log(P(Evento=NãoChover|Tempo=Sol)] $\n",
    "\n",
    "$= \\frac{2}{3}[ -\\frac{1}{2}*log(\\frac{1}{2}) - \\frac{1}{2}* log(\\frac{1}{2})] = \\frac{2}{3}*1 = \\frac{2}{3}$\n",
    "\n",
    "Ocorre em 2/3 na base (muito) e tem entropia alta 1!. Logo, tem um custo médio de informação de 2/3\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculamos -P(Tempo=Nublado)*$H(Evento|Tempo=Nublado)$ como \n",
    "\n",
    "$= P(Tempo=Nublado)*[ -P(Evento=Chover|Tempo=Nublado)*log(P(Evento=Chover|Tempo=Nublado) - P(Evento=NãoChover|Tempo=Nublado)*log(P(Evento=NãoChover|Tempo=Nublado)] $\n",
    "\n",
    "$= \\frac{1}{3}*[ -1*log(1) - 0* log(0)] = \\frac{1}{3}*0 = 0$\n",
    "\n",
    "Ocorre em 1/3 na base (razoável) e tem entropia 0!. Quando ele aparece, temos certeza! Logo, tem um custo médio de informação de 0! \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Conclusão para o atributo tempo \n",
    "\n",
    "Agora vamos calcular em média quanto cada valor de atributo tem de custo de infomação (Quanto menor em média, melhor)\n",
    "\n",
    "$H(Evento|Tempo) = \\frac{2}{3} + 0 = \\frac{2}{3} = 0.66$\n",
    "\n",
    "O que gerou impureza no atributo tempo foi a incerteza do $Tempo=Sol$!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### E qual será o valor de H(Evento|Umidade) ?\n",
    "\n",
    "Temos que $H(Evento|Umidade) = P(Umidade=Alta)*H(Evento|Umidade=Alta) $\n",
    "\n",
    "\n",
    "$= -P(Umidade=Alta)[ P(Evento=Chover|Umidade=Alta)*log(P(Evento=NãoChover|Umidade=Alta)] $\n",
    "\n",
    "$= 1*[ -\\frac{2}{3}*log(\\frac{2}{3}) - \\frac{1}{3}* log(\\frac{1}{3})] = 1*(0.91) = 0.91$\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Conclusão para o atributo umidade\n",
    "\n",
    "Só existe um valor de atributo para umidade e que ocorre em 100% da base (muito). Este atributo não ajuda a decidir com relação se vai chover ou não. Por isso, sua entropia é alta (incerteza alta) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ganho de Informação\n",
    "\n",
    "$IG(Y|X) = H(Y) - H(Y|X)$\n",
    "\n",
    "Quantifica o quanto de informação se ganha selecionando o atributo X como nó.\n",
    "Em outras palavras, quanto o atributo X reduz de entropia (incerteza) com relaçao à variável Y.\n",
    "\n",
    "Vejamos agora para o mesmo exemplo o $IG(Evento|Tempo)$ e $IG(Umidade|Tempo)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$IG(Evento|Tempo) = H(Evento) - H(Evento|Tempo) = [-(1/3)*log(1/3)-(2/3)*log(2/3)] - 0.66 = 0.91 - 0.66  = 0.25 $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$IG(Evento|Umidade) = H(Evento) - H(Evento|Umidade) = [-(1/3)*log(1/3)-(2/3)*log(2/3)] - 0.66 = 0.91 - 0.91  = 0 $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusão sobre qual o melhor atributo\n",
    "\n",
    "Veja que interessante $IG(Evento|Tempo)$ > $IG(Evento|Umidade)$, pois como vimos, umidade não agrega nada a informação do problema! Umidade possui incerteza tão alta quanto Tempo, não gerando nenhum ganho de informação. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
